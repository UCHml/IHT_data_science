{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Selection of data.**"
      ],
      "metadata": {
        "id": "Sp1rDT12X1eY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing modules, defending some functions, reading the data."
      ],
      "metadata": {
        "id": "ETeL6XkHaeZp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "md7aAnXNUSmE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
        "from sklearn.cluster import KMeans"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate entropy\n",
        "def calculate_entropy(data):\n",
        "    class_counts = data['class'].value_counts()\n",
        "    probabilities = class_counts / len(data)\n",
        "    entropy = -np.sum(probabilities * np.log2(probabilities))\n",
        "    return entropy\n",
        "\n",
        "# Function to calculate information gain\n",
        "def calculate_information_gain(data, attribute):\n",
        "    entropy_before = calculate_entropy(data)\n",
        "    values = data[attribute].unique()\n",
        "    entropy_after = 0\n",
        "\n",
        "    for value in values:\n",
        "        subset = data[data[attribute] == value]\n",
        "        entropy_after += len(subset) / len(data) * calculate_entropy(subset)\n",
        "\n",
        "    information_gain = entropy_before - entropy_after\n",
        "    return information_gain"
      ],
      "metadata": {
        "id": "D3DMLIxxI1wD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/sample_data/car.data\", names=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'class'])\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iukXL2ZXUv0",
        "outputId": "3464ad49-20e9-4762-8a40-be8971588666"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     buying  maint  doors persons lug_boot safety  class\n",
            "0     vhigh  vhigh      2       2    small    low  unacc\n",
            "1     vhigh  vhigh      2       2    small    med  unacc\n",
            "2     vhigh  vhigh      2       2    small   high  unacc\n",
            "3     vhigh  vhigh      2       2      med    low  unacc\n",
            "4     vhigh  vhigh      2       2      med    med  unacc\n",
            "...     ...    ...    ...     ...      ...    ...    ...\n",
            "1723    low    low  5more    more      med    med   good\n",
            "1724    low    low  5more    more      med   high  vgood\n",
            "1725    low    low  5more    more      big    low  unacc\n",
            "1726    low    low  5more    more      big    med   good\n",
            "1727    low    low  5more    more      big   high  vgood\n",
            "\n",
            "[1728 rows x 7 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Comments on reading. Short explanations about the need to transform and supplement data in certain columns:***\n",
        "\n",
        ">\n",
        "\n",
        "The resulting data is in tabular DataFrame  format with seven columns: '***buying***', '***maint***', '***doors***', '***persons***', '***lug_boot***', '***safety***' and '***class***'. Each row corresponds to a specific car.\n",
        "To prepare data for modeling, we can use the **Label Encoding** technique to replace categorical values with numeric ones."
      ],
      "metadata": {
        "id": "V2JNXkIpbDQw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Transformation of data**"
      ],
      "metadata": {
        "id": "7mrDKIdjrQ81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode categorical variables using LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "for col in df.columns:\n",
        "    df[col] = label_encoder.fit_transform(df[col])\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwwGsV5Fadh0",
        "outputId": "6d8aeeba-ceb8-4a57-a64a-9b44b0031f0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      buying  maint  doors  persons  lug_boot  safety  class  cluster\n",
            "0          3      3      0        0         2       1      2        1\n",
            "1          3      3      0        0         2       2      2        2\n",
            "2          3      3      0        0         2       0      2        1\n",
            "3          3      3      0        0         1       1      2        1\n",
            "4          3      3      0        0         1       2      2        2\n",
            "...      ...    ...    ...      ...       ...     ...    ...      ...\n",
            "1723       1      1      3        2         1       2      1        2\n",
            "1724       1      1      3        2         1       0      3        1\n",
            "1725       1      1      3        2         0       1      2        0\n",
            "1726       1      1      3        2         0       2      1        0\n",
            "1727       1      1      3        2         0       0      3        0\n",
            "\n",
            "[1728 rows x 8 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Comments on the conversion. Explanation of the need for standardization and normalization of data:***\n",
        "\n",
        "\n",
        "The resulting data were subjected to the **Label Encoding** process, where the categorical values of each attribute were replaced by their corresponding numerical equivalents. For example, '***buying***' has four possible categories (***v-high, high, med, low***), which are now represented by numerical values between 0 and 3. This transformation makes the data suitable for use in machine learning algorithms that require numerical input.\n",
        "\n",
        "Regarding standardization and normalization for the simple multilayer perceptron (**MLP**) (part 2):\n",
        "In the case of a simple **MLP**, standardization or normalization of the data may be important because the algorithm is sensitive to the values of the input attributes and their scales.\n",
        "Standardization can improve the speed of convergence of the algorithm and avoid the disproportionate impact of large values of individual attributes.\n"
      ],
      "metadata": {
        "id": "zwipx2nUcyOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardization and normalization of data\n",
        "scaler = StandardScaler()\n",
        "min_max_scaler = MinMaxScaler()\n",
        "\n",
        "df_scaled = pd.DataFrame(min_max_scaler.fit_transform(scaler.fit_transform(df)), columns=df.columns)\n",
        "\n",
        "print(df_scaled)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZFNe82sARnv",
        "outputId": "39add50d-a1a3-48f7-896e-2dfb19245931"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        buying     maint  doors  persons  lug_boot  safety     class  cluster\n",
            "0     1.000000  1.000000    0.0      0.0       1.0     0.5  0.666667      0.5\n",
            "1     1.000000  1.000000    0.0      0.0       1.0     1.0  0.666667      1.0\n",
            "2     1.000000  1.000000    0.0      0.0       1.0     0.0  0.666667      0.5\n",
            "3     1.000000  1.000000    0.0      0.0       0.5     0.5  0.666667      0.5\n",
            "4     1.000000  1.000000    0.0      0.0       0.5     1.0  0.666667      1.0\n",
            "...        ...       ...    ...      ...       ...     ...       ...      ...\n",
            "1723  0.333333  0.333333    1.0      1.0       0.5     1.0  0.333333      1.0\n",
            "1724  0.333333  0.333333    1.0      1.0       0.5     0.0  1.000000      0.5\n",
            "1725  0.333333  0.333333    1.0      1.0       0.0     0.5  0.666667      0.0\n",
            "1726  0.333333  0.333333    1.0      1.0       0.0     1.0  0.333333      0.0\n",
            "1727  0.333333  0.333333    1.0      1.0       0.0     0.0  1.000000      0.0\n",
            "\n",
            "[1728 rows x 8 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Standardization:**\n",
        "\n",
        "'***buying***', '***maint***', '***doors***', '***persons***', '***lug_boot***', '***safety***': After **Label Encoding** we have numerical values, but their scale can vary greatly.\n",
        "Standardization allows to rescale attributes so that their values have a mean of 0 and a standard deviation of 1.\n",
        "Formula: Standardized_Value = (X - Mean(X)) / StdDev(X), where X is the original value of the attribute, Mean(X) is the average value of the attribute, StdDev(X) is the standard deviation of the attribute.\n",
        "\n",
        "**Normalization (optional):**\n",
        "\n",
        "Normalization can also be used to bring values between 0 and 1.\n",
        "Formula: Normalized_Value = (X - Min(X)) / (Max(X) - Min(X)), where X is the original value of the attribute, Min(X) is the minimum value of the attribute, and Max(X) is the maximum value of the attribute."
      ],
      "metadata": {
        "id": "YHyRKYuWeMuE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Dividing samples into clusters and creating a sample**"
      ],
      "metadata": {
        "id": "_GCq89U_jhha"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Discussion of possible options for clustering:***\n",
        "\n",
        "**K-Means clustering:**\n",
        "\n",
        "**Pros**: Simple and computationally efficient, works well with spherical clusters.\n",
        "Cons: Sensitive to initial placement of centroids, assumes clusters have similar variances.\n",
        "\n",
        "**Hierarchical clustering:**\n",
        "\n",
        "Pros: captures hierarchical relationships in the data, does not assume a fixed number of clusters.\n",
        "Cons: Can be computationally expensive, difficult to interpret for large datasets.\n",
        "\n",
        "**DBSCAN (Spatial Clustering of Density-Based Noise Applications):**\n",
        "\n",
        "Pros: Can detect clusters of arbitrary shape, resistant to outliers.\n",
        "Cons: Sensitive to the choice of hyperparameters, can struggle with clusters of different densities.\n",
        "I will use the **K-Means** clustering in the program."
      ],
      "metadata": {
        "id": "NORadhApjlbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply K-means clustering\n",
        "num_clusters = 3\n",
        "kmeans = KMeans(n_clusters=num_clusters, n_init=10, random_state=42)\n",
        "df['cluster'] = kmeans.fit_predict(df_scaled.drop('class', axis=1))\n",
        "\n",
        "# Display clusters in the original DataFrame\n",
        "print(\"\\nDataFrame with Clusters:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEzxuwVPWZAy",
        "outputId": "a8a6b932-ffd5-42f8-da2e-321e573dcb27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DataFrame with Clusters:\n",
            "   buying  maint  doors  persons  lug_boot  safety  class  cluster\n",
            "0       3      3      0        0         2       1      2        2\n",
            "1       3      3      0        0         2       2      2        0\n",
            "2       3      3      0        0         2       0      2        2\n",
            "3       3      3      0        0         1       1      2        2\n",
            "4       3      3      0        0         1       2      2        0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Comments on the obtained clusters and sample:***\n",
        "\n",
        "So the clustering has been successfully applied and the \"***cluster***\" column has been added to the DataFrame. Each row is assigned a cluster label (in this case 0, 1, or 2). The resulting DataFrame now contains both the original attributes and the assigned cluster labels.\n",
        "\n"
      ],
      "metadata": {
        "id": "fabBPM-kl6MD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Data compression using the method of principal components**"
      ],
      "metadata": {
        "id": "j9lMgPucfTXQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Description of the data compression algorithm using the method of principal components:***\n",
        "\n",
        "• Data standardization\n",
        "\n",
        "• Construction of the covariance matrix\n",
        "\n",
        "• Finding eigenvalues and eigenvectors of the covariance matrix\n",
        "\n",
        "• Sorting of eigenvalues by magnitude in descending order of absolute value\n",
        "\n",
        "• Selection of ***k<d*** eigenvectors for the ***k*** largest eigenvalues\n",
        "\n",
        "• Construction of the projection matrix"
      ],
      "metadata": {
        "id": "YAas3xYSfJ95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Covariance matrix\n",
        "covariance_matrix = np.cov(df_scaled, rowvar=False)\n",
        "\n",
        "# Eigenvalues and eigenvectors\n",
        "eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n",
        "\n",
        "# Print eigenvectors\n",
        "print(\"\\nEigenvectors:\")\n",
        "for i, eigenvector in enumerate(eigenvectors):\n",
        "    print(f\"PC{i + 1}:\", eigenvector)\n",
        "\n",
        "# Perform PCA using the first two principal components\n",
        "pca_result = df_scaled.dot(eigenvectors[:, :2])\n",
        "\n",
        "# Display PCA result\n",
        "print(\"\\nPCA Result (First Two Principal Components):\")\n",
        "print(pca_result.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxIp8icDjpaT",
        "outputId": "53976e98-3e14-48b2-c757-388f96e7f100"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Eigenvectors:\n",
            "PC1: [-3.91493594e-04 -5.56935493e-03  7.65372078e-02 -4.81091183e-02\n",
            "  6.74767000e-01  5.20258323e-01  5.15574602e-01  3.62181907e-17]\n",
            "PC2: [-2.66127672e-02  3.47391179e-02  6.64939076e-02 -3.82243408e-02\n",
            "  5.98432772e-01 -2.41675976e-12 -7.96291800e-01 -3.40151553e-16]\n",
            "PC3: [ 2.38496097e-04  3.39282542e-03 -4.66261151e-02  2.93078537e-02\n",
            " -4.11064954e-01  8.54008945e-01 -3.14085677e-01 -1.58476289e-15]\n",
            "PC4: [-9.17857653e-02  1.22224005e-01 -3.31780639e-01  9.19516361e-01\n",
            "  9.69523263e-02  4.88545134e-14  9.41712420e-03 -1.07515115e-01]\n",
            "PC5: [-6.40132993e-01  6.30039912e-01  1.34065972e-01 -1.40472302e-01\n",
            " -4.53859394e-02  9.43518823e-14  3.27095407e-02 -3.90440150e-01]\n",
            "PC6: [-2.84145052e-01  2.83414272e-01  1.82355692e-02  4.81401059e-02\n",
            " -7.98033837e-03  4.72961311e-14  1.50751049e-02  9.14328710e-01]\n",
            "PC7: [-1.16142472e-02  1.10527874e-01 -9.24031152e-01 -3.57473991e-01\n",
            "  7.75580569e-02  2.80887523e-14  3.49592545e-03  1.65479274e-16]\n",
            "PC8: [-7.07264713e-01 -7.03080085e-01 -7.29961530e-02 -4.18720160e-03\n",
            "  7.39869030e-03 -2.20574955e-14 -7.36945737e-03 -3.68560614e-17]\n",
            "\n",
            "PCA Result (First Two Principal Components):\n",
            "          0         1\n",
            "0 -1.170585  0.523062\n",
            "1 -1.666290  0.313229\n",
            "2 -1.028512  0.381355\n",
            "3 -0.850518  0.208042\n",
            "4 -1.346223 -0.001791\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Comments on compression results:***\n",
        "\n",
        "Principal Component Analysis (**PCA**) is applied to the vehicle dataset to reduce its dimensionality. The program first standardizes and normalizes the data, then calculates the covariance matrix and outputs the eigenvectors and eigenvalues. The eigenvectors represent the principal components that capture the maximum variance in the data. The program outputs these eigenvectors and performs **PCA**, projecting the data onto the first two principal components. The resulting **PCA** coordinates are displayed, providing a lower-dimensional representation of the original dataset."
      ],
      "metadata": {
        "id": "EY0OTbxgf-zK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Calculation of entropy and assessment of informativeness of sample attributes**"
      ],
      "metadata": {
        "id": "VY8qBFP_hFg7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "***Discussion of possible options for selecting the target attribute.***\n",
        "***Formulas for calculating entropy and information gain for processed data:***\n",
        "\n",
        ">\n",
        "\n",
        "**Calculation of entropy:**\n",
        "\n",
        ">\n",
        "\n",
        "The entropy (H(S)) for a set S with c classes is calculated as the sum of the negative products of the fraction of instances (p_i) and the base 2 logarithm of p_i:\n",
        "\n",
        ">\n",
        "\n",
        "Entropy (S) = - sum from i=1 to c (p_i * log2(p_i))\n",
        "\n",
        ">\n",
        "\n",
        "**Calculation of information amplification:**\n",
        "\n",
        "The information gain (IG(S, A)) for the attribute A and the set S is calculated by subtracting the sum of the weighted entropies of the subsets (S_v) from the entropy of the original set:\n",
        "\n",
        "IG(S, A) = Entropy(S) – the sum of v in values(A) from ((|S_v| / |S|) * Entropy(S_v) )\n",
        "\n",
        "**Where:**\n",
        "\n",
        "Values (A) are the unique values of attribute A.\n",
        "|S| is the total number of instances in the set S.\n",
        "|С_в| is the number of instances in set S with value v for attribute A.\n",
        "Entropy(S_v) is the entropy of a subset of S_v.\n",
        "\n",
        ">\n",
        "\n",
        "https://colab.research.google.com/drive/1ExiTy--Tsq4D56sZyvYEIryYZ67lKxKS#scrollTo=D3DMLIxxI1wD&line=1&uniqifier=1\n",
        "\n",
        ">\n",
        "\n",
        "Select the target attribute:\n",
        "\n",
        "The choice of the target attribute depends on the goal of the problem and the required information. In a vehicle evaluation data set, the '***class***' attribute may be a suitable object because it represents the vehicle's eligibility.\n",
        "Entropy and information gain:\n",
        "A lower entropy indicates a cleaner or more homogeneous set. A high information gain indicates that the attribute effectively partitions the data into subsets with greater homogeneity relative to the target attribute.\n",
        "In summary, the goal is to select a target attribute that leads to an optimal distribution, resulting in a model that effectively classifies instances."
      ],
      "metadata": {
        "id": "_iqMUpcvgH1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display information gain for each attribute excluding the target attribute 'class'\n",
        "target_attribute = 'class'\n",
        "for attribute in df.columns[:-1]:  # Exclude the target attribute 'class'\n",
        "    if attribute != target_attribute:\n",
        "        information_gain = calculate_information_gain(df, attribute)\n",
        "        print(f'Information Gain for {attribute}: {information_gain}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoyqQqS2mL1o",
        "outputId": "a204ba29-03e4-4eee-cc08-3710a3a1fd0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Information Gain for buying: 0.09644896916961399\n",
            "Information Gain for maint: 0.07370394692148596\n",
            "Information Gain for doors: 0.004485716626632108\n",
            "Information Gain for persons: 0.2196629633399082\n",
            "Information Gain for lug_boot: 0.030008141247605424\n",
            "Information Gain for safety: 0.262184356554264\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Comments on the value of sample attributes:***\n",
        "\n",
        "Information Gain for buying: 0.0964\n",
        "\n",
        "Information Gain for maint: 0.0737\n",
        "\n",
        "Information Gain for doors: 0.0045\n",
        "\n",
        "Information Gain for persons: 0.2197\n",
        "\n",
        "Information Gain for lug_boot: 0.0300\n",
        "\n",
        "Information Gain for safety: 0.2622\n",
        "\n",
        "'***persons***' and '***safety***' have relatively higher values of information gain, indicating that these features are more informative for distinguishing different classes."
      ],
      "metadata": {
        "id": "Cs3YaZkChi5C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. General conclusions of the first part of work:**"
      ],
      "metadata": {
        "id": "n1986GMyobxT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**K-means** clustering revealed patterns in the data, assigning each sample to one of three clusters.\n",
        "**PCA** was used to reduce the dimensionality of the data set while preserving information.\n",
        "**Information acquisition** analysis helps identify the attributes that contribute most to the classification task.\n",
        "Feature '***cluster***' provides an additional view of the data set by grouping similar samples.\n",
        "\n",
        "Overall, the program offers an understanding of the structure of the vehicle evaluation data set, providing a framework for further analysis and decision-making. The combination of **PCA** and **K-means** clustering enriches the understanding of patterns and relationships in the data."
      ],
      "metadata": {
        "id": "PLcgVF0ToRrj"
      }
    }
  ]
}